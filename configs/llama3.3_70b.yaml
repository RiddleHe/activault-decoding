run_name: llama3.3_70b
num_runs: 1 # Increase this!
transformer_config:
  model_name: meta-llama/Llama-3.3-70B-Instruct
  dtype: float16
  cache_dir: /cache
  max_per_device_memory: 55GB
data_config:
  bucket_name: main
  data_key: infinity-instruct:CHAT_TEMPLATE
  n_batches: 256 # 250000
  seq_length: 2048
  batch_size: 4
  seed: 42
  skip_cache: False
  start_batch: 0 
  clean_added_tokens: True
  clean_default_system_prompt: True
upload_config:
  batches_per_upload: 32
  hooks: ["blocks.24.hook_resid_post", "blocks.36.hook_resid_post", "blocks.48.hook_resid_post", "blocks.56.hook_resid_post"]
